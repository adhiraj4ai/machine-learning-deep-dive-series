{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e24de5b7f411a49b",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a fundamental statistical and machine learning technique used to model the relationship between variables. It's a simple yet powerful algorithm widely adopted in data science and predictive analytics. Its foundation for statistical learning and machine learning. It models the relationship between one or more independent variables (predictors) and a dependent variable (response) by fitting a linear equation to observed data.\n",
    "\n",
    "Linear regression aims to model the relationship between variables by assuming that the dependent variable  Y  is a linear combination of the independent variables  $x_1, x_2, \\dots ,x_n$ , plus some error term $\\epsilon$ .\n",
    "\n",
    "The formula for simple linear regression (one predictor) is:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $y$ is the dependent variable (what you’re trying to predict).\n",
    "* $x$ is the independent variable (the predictor).\n",
    "* $\\beta_0$ is the intercept (the value of $y$ when $x = 0$).\n",
    "* $\\beta_1$ is the slope of the line (the change in $y$ for a one-unit change in $x$ ).\n",
    "* $\\epsilon$ is the error term (the difference between the observed value of $y$ and the value predicted by the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1846e98ccd89e4",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression\n",
    "Linear regression comes with several key assumptions as explained by statistics:\n",
    "\n",
    "**i. Linearity:**\n",
    "\n",
    "The relationship between the independent variables and the dependent variable is assumed to be linear. This means that the effect of the independent variable on the dependent variable is constant across the range of the data. Mathematically, the model is:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + … + \\beta_n x_n + \\epsilon\n",
    "$$\n",
    "\n",
    "**ii. Independence of Errors:**\n",
    "\n",
    "The residuals (errors  \\epsilon ) should be independent of each other. This means that there should be no correlation between consecutive errors. In time series data, this assumption implies that there is no autocorrelation.\n",
    "\n",
    "**iii. Homoscedasticity:**\n",
    "\n",
    "The variance of the residuals  \\epsilon  should be constant across all levels of the independent variables. This assumption ensures that the spread of the residuals is the same, regardless of the value of the predictors. If the errors have varying variance (i.e., heteroscedasticity), the model may be inefficient.\n",
    "\n",
    "**iv. Normality of Errors:**\n",
    "\n",
    "The errors  \\epsilon  should be normally distributed, especially important for hypothesis testing. This assumption ensures that the t-tests and F-tests used for significance testing in the regression model are valid.\n",
    "\n",
    "v. No Multicollinearity (in multiple regression):\n",
    "\n",
    "In the case of multiple predictors, the independent variables should not be too highly correlated with each other. High correlation between predictors (multicollinearity) can make it difficult to estimate the individual effects of each variable.\n",
    "\n",
    "***Interpretation:***\n",
    "In machine learning, the assumptions of linear regression are often not strictly met, yet they still play an important role in ensuring the validity of the model and cannot be entirely ignored. Traditionally, statistical methods applied linear regression to smaller datasets, where meeting these assumptions was critical for the model’s accuracy and interpretability. However, with the advent of big data, where vast amounts of data are readily available in repositories such as databases, data warehouses, cloud storage, and data lakes, these assumptions are sometimes overlooked.\n",
    "\n",
    "Today, many machine learning engineers, when applying linear regression, may be unaware of or pay little attention to these assumptions. Often, the focus is placed on model building, performance metrics, and deployment rather than on validating the model and interpreting its outputs. In some cases, due to the sheer volume of data, engineers might assume that these assumptions can be safely ignored, as large datasets tend to mask issues such as normality of errors or homoscedasticity. However, overlooking these assumptions can lead to biased, inefficient, or poorly generalized models, especially when the goal is not only prediction but also understanding the relationships between variables.\n",
    "\n",
    "Furthermore, ignoring assumptions can hinder model interpretability and affect decision-making in domains where understanding variable relationships is essential, such as healthcare, finance, or scientific research. Even with large datasets, validation techniques such as residual analysis, cross-validation, and regularization should be used to identify potential violations and mitigate their impact.\n",
    "\n",
    "Modern machine learning tools and techniques—such as regularization (e.g., Lasso, Ridge), ensemble methods, and automated model diagnostics—can help address violations of these assumptions. However, it’s still important for practitioners to understand the theoretical underpinnings of linear regression, as this allows for better model design, more meaningful interpretation, and improved outcomes, especially in high-stakes applications.\n",
    "\n",
    "Thus, while large datasets offer some flexibility, the assumptions of linear regression remain crucial for ensuring robust, interpretable, and reliable models. Balancing the technical aspects of model building with thorough validation practices is key to avoiding pitfalls and maximizing the value of the data. Therefore, it is important to carefully consider these assumptions and their interpretation. Also, soemtimes due to the large volume of data these assumptions are ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f0372fed0b66b",
   "metadata": {},
   "source": [
    "## Types of Linear Regression\n",
    "Basically, there are two type of linea regression\n",
    "\n",
    "* Single Variable Linear Regression\n",
    "* Multiple Variable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e291cbe4c68ca60f",
   "metadata": {},
   "source": [
    "### Single Variable Linear Regression\n",
    "It is often known as simple linear regression. Simple Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. In this, there's a single input variable ($x$) and a single output variable ($y$). One variable is considered explanatory (Independent Variables or Predictors), while the other is dependent (Response Variable) . For instance, a researcher might use linear regression to relate individuals' weights to their heights. The relationship between these variables is represented by a straight line (linear line), often referred to as the \"best fit line\" or \"regression line.\"\n",
    "\n",
    "\n",
    "This line is described by the equation:\n",
    "$$ y = \\beta_0 + \\beta_1 x $$\n",
    "\n",
    "Where:\n",
    "- $y$ is the dependent variable\n",
    "- $x$ is the independent variable\n",
    "- $\\beta_0$ is the y-intercept\n",
    "- $\\beta_1$ is the slope of the line\n",
    "\n",
    "Using simple linear regression, we can answer the following questions:\n",
    "1. ***Is there a relationship between the dependent and independent variables?***\n",
    "Yes, if the slope $\\beta_1$ is significantly different from zero, it indicates a relationship. For instance, if \\beta_1 = 0.5, it means that for every unit increase in the independent variable (e.g., height), the dependent variable (e.g., weight) increases by 0.5 units on average.\n",
    "2. ***How strong is the relationship between the dependent and independent variables?***\n",
    "The strength of the relationship can be assessed by the magnitude of $\\beta_1$ and the coefficient of determination ($R^2$). A larger absolute value of $\\beta_1$ or a higher $R^2$ indicates a stronger relationship.\n",
    "3.\t***What is the association between the dependent and independent variables?***\n",
    "The slope $\\beta_1$ quantifies the association. For example, if $\\beta_1 = 0.5$, it suggests that for each additional centimeter in height, weight increases by 0.5 kilograms on average.\n",
    "4.\t***How accurately can we predict the dependent variable with unseen data?***\n",
    "The accuracy of predictions on unseen data can be evaluated using metrics such as the mean squared error (MSE) or the root mean squared error (RMSE) on a test dataset. Lower values of these metrics indicate more accurate predictions.\n",
    "5.\t***Is the relationship linear?***\n",
    "The model assumes a linear relationship between height and weight, expressed by the equation $Weight = \\beta_0 + \\beta_1 * Height + ε$. A linear relationship means that the change in weight is proportional to the change in height.\n",
    "\n",
    "Thus, the goal of simple linear regression is to find the optimal values for the intercept ($\\beta_0$) and slope ($\\beta_1$) that minimize the difference between the predicted and actual values. Simple linear regression provides a way to understand and quantify the relationship between height and weight, allowing us to make predictions and assess the strength and nature of this relationship in a straightforward manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16b407e2adf9f0",
   "metadata": {},
   "source": [
    "### Multiple Variable Linear Regression\n",
    "Multiple Variable Linear Regression, often referred to as multiple linear regression (MLR) or multivariate linear regression, is a statistical method used to model the relationship between one dependent variable and two or more independent variables. It extends the concept of simple linear regression, which deals with only one independent variable.\n",
    "\n",
    "The primary goal of multiple linear regression is to predict the value of the dependent variable (also known as the response or target variable) based on the values of the independent variables (also called predictors, features, or explanatory variables). This method assumes a linear relationship between the dependent and independent variables.\n",
    "\n",
    "Mathematical Formulation\n",
    "\n",
    "The mathematical expression for multiple linear regression is given by the following equation:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the dependent variable (the value we aim to predict).\n",
    "- $x_1, x_2, \\dots, x_n$ are the independent variables (the predictors).\n",
    "- $\\beta_0$  is the intercept (the constant term).\n",
    "- $\\beta_1, \\beta_2, \\dots, \\beta_n$ are the coefficients (slopes), representing the effect of each independent variable on the dependent variable.\n",
    "- $\\epsilon$  is the error term, accounting for the deviation between the observed and predicted values.\n",
    "\n",
    "For a dataset with  $m$  observations and  $n$  features, the model can be expressed in matrix form as:\n",
    "\n",
    "$$\n",
    "y = X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y \\in \\mathbb{R}^{m \\times 1}$ is the vector of dependent variable values.\n",
    "- $X \\in \\mathbb{R}^{m \\times (n + 1)}$ is the matrix of independent variables, with  $x_0 = 1$  to account for the intercept term.\n",
    "- $\\beta \\in \\mathbb{R}^{(n + 1) \\times 1}$ is the vector of coefficients.\n",
    "- $\\epsilon \\in \\mathbb{R}^{m \\times 1}$ is the vector of error terms.\n",
    "\n",
    "This matrix form allows for a more compact representation of the model and facilitates efficient computation, especially when working with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835855b2709f358c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
